# Методология дообучения canary-1b-v2 под RU (обновлено по последним публикациям, с проверенными ссылками)

Этот документ — готовая секция для training/Методология.md. Включает уточнённый состав датасетов с HF, пропорции смешивания, частичную разморозку (Stage 1, ~1200 ч), LoRA-адаптацию (Stage 2, 22 ч домен + 6 ч голоса), а также практики отбора данных и валидации. Все рекомендации проверены на актуальных источниках (HF/NeMo/Riva, 2024–2025).

## 0) Контекст и что изменилось

- **Canary-1b-v2**: официальный model card обновлён (авг. 2025) — есть детали по архитектуре (32 слоя энкодера FastConformer + 8 слоёв декодера), 3-стадийной тренировке, temperature sampling (τ = 0.5) по языкам/корпусам и наличию 36 000 ч не-речи в обучении. Это напрямую подтверждает те приёмы, которые мы используем в рецепте (τ-сэмплинг, добавка non-speech, long-form через динамический чанкинг). 
- **Russian LibriSpeech** теперь доступен на HF: `bond005/rulibrispeech`. В предыдущей версии плана мы предположили отсутствие RU-варианта LibriSpeech на HF — исправлено, RLS включён в основной микс.

## 1) Данные и пропорции (целевой микс ≈ 1200 ч)

### 1.1 Базовые источники с HF

| Роль | Датасет (HF) | Почему берём |
|---|---|---|
| Чтение/крауд, дальнее поле | `SberDevices/Golos` | Большой качественный корпус (~1240 ч), есть крауд и farfield — важен для устойчивости. |
| Крауд-речь с акцентами | `mozilla-foundation/common_voice_17_0` (конфиг `ru`) | Разнообразные дикторы/условия, регулярный бенчмарк. |
| Чистое чтение (RU) | `bond005/rulibrispeech` | Аналог LibriSpeech для RU (на HF), полезен для дикции/орфографии. |
| Разговорные подкасты | `bond005/podlodka_speech` | Живая спонтанная речь/лонг-форм; будем дополнительно нарезать окнами. |
| Телефонные диалоги | `UniDataPro/russian-speech-recognition-dataset` | ~338 ч телефонных диалогов — другой канал/частоты/дикция. |
| Non-speech для анти-галлюцинаций/миксинга | `bond005/audioset-nonspeech` | Готовый отфильтрованный нес-пич под аугментацию; обновлялся совсем недавно. |
| (dev/test, а не train) | `google/fleurs` (`ru_ru`) | Малые, стандартизованные фразы — хороши для валидации. |

**Примечание.** MLS (`facebook/multilingual_librispeech`) не содержит русский — держим его вне train-микса под RU. В карточке Canary он фигурирует как часть их мульти-язычного пула, но нам для RU-дообучения он не нужен.

### 1.2 Рекомендуемые доли по шагам (Stage 1)

- 85 % **RU_MIX** / 15 % ваш домен (22 ч, без 6 ч голосов)
- Внутри RU_MIX (85 %):
  - Golos — 35 %
  - Common Voice (ru) — 25 %
  - Russian LibriSpeech — 15 %
  - Podlodka — 10 %
  - Телефонные диалоги (UniDataPro) — 10 %
  - Non-speech (AudioSet-nonspeech) — 5 % (как вероятность шага/подмешивания; не как «часы»)

Почему так: покрываем чтение/крауд/дальнее поле, разговорный и телефонный каналы + немного нес-пич; 15 % домена «подруливают» модель к вашим текстам без риска «переписать» общую устойчивость. Сэмплинг с температурой τ = 0.5 — ровно как в Canary, чтобы редкие домены не исчезали.

## 2) Подготовка данных (best practices)

- Аудио: WAV/FLAC моно, 16 кГц; нормализовать уровень (пик ≈ −1 dBFS).
- Длина сегмента: 1–35 с (лонг-форму предварительно VAD+нарезка окнами 15–30 с, перекрытие 5–10 %). Canary для инференса использует dynamic chunking с 1 с оверлэпом — держим совместимость.
- Фильтрация качества: отбрасывать SNR < 5–7 дБ; удалять пустые/зашумлённые клипы; править ломанный текст. (SNR/фильтры поддерживаются в Lhotse.)
- Текст-норма: единые правила чисел/единиц/регистра/«ё/е», сохраняем пунктуацию/капитализацию (в Canary все транскрипты с пунктуацией/капсом).
- Дедуп: по нормализованному тексту и простым аудио-сигнатурам; переспикерные капы (≤ 60 мин на диктора в train), чтобы избежать переобучения на 1–2 голосах.

## 3) Аугментации

- **Speed perturbation:** 0.9 / 1.0 / 1.1 (равновероятно).
- **SpecAugment (умеренно):** 2 time-mask (T≈30–40 фреймов), 2 freq-mask (F≈25–30 каналов).
- **Шумы/реверб:** MUSAN + RIRS_NOISES, SNR 0…20 дБ, p≈0.3–0.5. Учтите: избыточный шум может ухудшить точность на чистой речи — держим баланс.
- **Non-speech (AudioSet-nonspeech):** 2–5 % шагов — как чистые «тишина/шум/музыка» (если токен `<no_speech>` поддерживается) либо как фон для миксинга. В Canary в трейне было 36 000 ч не-речи — это подтверждает пользу такого сигнала.

## 4) Stage 1 — частичная разморозка (≈ 1200 ч, A6000 48 GB)

### 4.1 Что размораживаем

- Decoder — обучаем полностью.
- Encoder (FastConformer) — верхние 8 слоёв из 32 (≈ верхняя треть) + обновление BN-статистик в соответствующих conv-блоках; нижние 24 слоя остаются замороженными. Такая политика соответствует практике частичной разморозки в ASR (фиксировать нижнюю «акустическую» часть и адаптировать «высокие» уровни/декодер).

### 4.2 Сэмплинг

- По корпусам: вероятности согласно п. 1.2, τ = 0.5 (как в Canary).
- По длительности: duration-based bucketing (Lhotse/NeMo), таргет на шаг ≈ 180 с аудио/GPU; меньше паддинга — стабильнее VRAM/скорость.

### 4.3 Оптимизация (1×A6000, 48 GB)

- Precision: BF16/FP16 + grad-checkpointing.
- Эффективный шаг: `max_duration_per_gpu≈180 с × grad_accum=4 ⇒ ~720 с` аудио/шаг. Для 1200 ч (4 320 000 с) это ≈ 6 000 шагов на «эпоху-эквив.» (порядок величины).
- Оптимизатор: AdamW, weight decay 0.01.
- Дифференцированный LR:
  - Decoder: 5e-5…1e-4
  - Unfrozen encoder (top-8): 1e-5…3e-5
- Scheduler: cosine с warmup = 5–10 % шагов; ранний стоп по сводной dev-метрике (домен + общий RU).
- Dropout: ≈ 0.1 в attention/FFN; SpecAugment — как в п. 3.

### 4.4 Валидация и таргеты

- Dev/test наборы:
  - CV17(ru) test
  - Golos test (crowd + farfield)
  - FLEURS (`ru_ru`)
- Метрики: WER/CER (вариант без пунктуации + с пунктуацией).
- Шумоустойчивость: оценка WER при SNR = 10/5/0 дБ (MUSAN-миксы), по аналогии с разделом Noise Robustness в карточке Canary.
- Ожидания (правило-оф-томб): после Stage 1 — заметное улучшение на домене без деградации > ~5 % относ. на CV/Golos.

## 5) Stage 2 — LoRA (22 ч домен + 6 ч целевые голоса)

### 5.1 Куда ставить LoRA

- Decoder: self-attn (Q,K,V,O) + FFN — обязательно.
- Encoder (верхние 2–4 слоя) — рекомендуется при явном сдвиге акустики/терминологии.
- Гиперпараметры LoRA для ~1B AED: rank = 16, alpha = 32, dropout = 0.05 — проверенный старт; веса базы заморожены. (PEFT/LoRA в NeMo: документация и поддерживаемые методы.)

### 5.2 Обучение и анти-забывание

- Оптимизатор: AdamW, LR = 2e-4…3e-4, warmup = 5 %, cosine, wd = 0.01.
- Смесь данных на шаг:
  - 80–90 % — ваши 22 ч доменных + 6 ч целевых голосов (овэр-сэмпл спикеров в финальные 20–30 % шагов ×2–3);
  - 10–20 % — «rehearsal» из узкого RU-среза (например, CV/Golos dev) во избежание catastrophic forgetting.
- Регуляризация (опционально): лёгкий KD от чекпойнта Stage 1 на rehearsal-миксе (λ≈0.2). В исследованиях по CL показывается, что rehearsal+KD стабилизируют качество вне домена.
- Early-stopping: сводная метрика 60 % (domain-dev) + 40 % (general-ru dev).

## 6) Отбор 1200 ч и активное обучение

Когда сырья больше, чем 1200 ч, применяем комбинированный скоринг и выбираем top-N:

- ASR-confidence от сильной teacher-модели (например, Canary) ⇒ фильтруем низкую уверенность/высокий surrogate-WER. Проверенная практика для ASR data selection.
- SNR/длина/энергетика ⇒ оставляем «здоровую» акустику и сбалансированные длины.
- LM-перплексия по нормализованному тексту ⇒ отбрасываем «мусор». (Широко используется в фильтрации ASR-корпусов и недавних работах по улучшению данных.)
- Диверсификация по типу канала (чтение/крауд/подкаст/телефон) и спикерам.
- Active Learning (опционально): добираем наименее уверенные и наиболее непохожие примеры для ручной верификации; есть открытые инструменты (напр., Cleanlab ActiveLab), которые можно встроить в ваш пайплайн отбора.

## 7) Ресурсы и режимы (A6000 48 GB)

| Параметр | Рекомендация |
|---|---|
| Mixed precision | BF16/FP16 + grad-checkpointing |
| Bucketing | duration-based, ~20–30 бакетов, `max_duration_per_gpu≈180 с` |
| Grad accumulation | 4 (эффективно ~720 с/шаг на 1 GPU) |
| «Эпоха-эквив.» | ~6 000 шагов на 1200 ч (оценка; уточняется логами sampler) |
| Аугментации | speed 0.9/1.0/1.1; SpecAugment умеренный; MUSAN/RIRS; 2–5 % non-speech |
| Останов | ранний стоп по сводной dev-метрике |

(Параметры соответствуют рекомендуемым конфигурациям NeMo/Riva; см. разделы по SpecAugment, шум-аугментации и PEFT.)

## 8) Валидация: что и как считать

- Основные тесты: CV17(ru) test, Golos test (crowd + farfield), FLEURS (`ru_ru`). Отчёт: WER/CER по двум режимам (без/с пунктуацией) + таблица WER при SNR = 10/5/0 дБ.
- Long-form: sanity-check на длинных подкастах (напр., Podlodka), убедиться, что чанкинг не рвёт контекст (Canary поддерживает dynamic chunking).

## 9) Итоговые чек-листы для агентов

### Data-Agent

1. Выгрузить и привести к одному формату: Golos, CV17(ru), RuLibriSpeech, Podlodka, UniDataPro (телефония), AudioSet-nonspeech; FLEURS — только dev/test.
2. Фильтры: длина 1–35 с; SNR ≥ 5–7 дБ; нормализация текста с пунктуацией; дедуп и капы по спикерам.
3. Селектор 1200 ч: confidence + SNR/длина + LM-PP + диверсификация; (опц.) ActiveLab.

### Train-A-Agent (Stage 1, частичная разморозка)

1. Unfreeze: decoder + top-8 encoder, BN-статы — обновлять.
2. Семплинг: доли из п. 1.2; τ = 0.5.
3. Aug: speed, SpecAugment, MUSAN/RIRS; 2–5 % non-speech.
4. Оптимизация: AdamW; LR-decoder 5e-5…1e-4, LR-encoder 1e-5…3e-5; warmup = 5–10 %; cosine; ранний стоп.

### Train-B-Agent (Stage 2, LoRA)

1. Внедрить LoRA: decoder (+верх encoder 2–4 слоя при необходимости); r=16, α=32, dropout=0.05.
2. Данные: 80–90 % домен+голоса / 10–20 % rehearsal (CV/Golos dev); (опц.) KD = 0.2.
3. Early-stopping по сводной метрике (60 % domain-dev + 40 % general-ru dev).

### Eval-Agent

1. WER/CER на CV17/Golos/FLEURS + domain-dev; шумоустойчивость SNR = 10/5/0; long-form sanity-check.

## 10) Ссылки (основные)

- Canary-1b-v2 (HF, обновлённый model card): архитектура 32+8, τ = 0.5, 36 000 ч non-speech, dynamic chunking, бенчмарки.
- Датасеты (HF): Golos; Common Voice 17; Russian LibriSpeech; Podlodka; UniDataPro RU-телефония; AudioSet-nonspeech; FLEURS.
- Granary (HF): публичная карточка датасета, используемая Canary.
- NeMo / Riva доки: SpecAugment/аугментации; PEFT/LoRA; fine-tuning/адаптеры; замечания о шум-аугментации и её влиянии на чистую речь.
- Lhotse: рецепты и загрузка/бакетинг (HF-хостинг FLEURS; duration-based sampling).
- Отбор данных: confidence/PP-фильтрация и современные обзоры.

## Приложение A. Быстрый «order of magnitude» по времени

На 1×A6000/48 GB при `max_duration_per_gpu≈180 с`, `grad_accum=4` и «эпохе-эквив.» ~6 000 шагов — ориентируйтесь на 60–100k шагов (1–2 прохода по выборке с аугментациями/температурным сэмплингом) с ранним стопом. Фактическая скорость зависит от VAD-нарезки/аугментаций/IO.

## Вывод

План подтверждён последним model card Canary-1b-v2 (τ-сэмплинг, non-speech, long-form) и актуальными практиками NeMo/Riva.

RU-микс скорректирован и включает Russian LibriSpeech с HF.

Для Stage 1 — частичная разморозка (decoder + top-8 encoder), мягкие аугментации и rehearse-friendly валидация; для Stage 2 — LoRA с rehearsal/KD, акцент на 6 ч целевых голосов. Это даёт сильную доменную прибавку при сохранении общей устойчивости.
